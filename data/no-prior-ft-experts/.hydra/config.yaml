mode: prod
critic:
  type: causal
  name: google/gemma-3-4b-it
  device: cuda
  is_openrouter: false
experts:
  num_experts: 3
  type: seq2seq
  name: google/flan-t5-small
  device: cuda
  debate_rounds: 5
  feedback_size: 100
  batch_size: 10
model_params:
  max_new_tokens: 256
  temperature: 0.7
  do_sample: true
  top_p: 0.9
  num_return_sequences: 1
  min_new_tokens: 10
lora:
  enabled: true
  r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  bias: none
training:
  output_dir: ./results
  eval_strategy: steps
  save_strategy: 'no'
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4
  eval_accumulation_steps: 4
  eval_steps: 500
  max_steps: 200
  logging_steps: 20
  learning_rate: 1.0e-05
  weight_decay: 0.01
data:
  data_cache_dir: ./data/cache
  category: summarization
  name: samsum
  split:
  - train[:10%]+validation[:5%]+test[:5%]
